<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Optimizers · ComplexSPSA.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">ComplexSPSA.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Introduction</a></li><li class="is-active"><a class="tocitem" href>Optimizers</a><ul class="internal"><li><a class="tocitem" href="#First-order-optimizers"><span>First-order optimizers</span></a></li><li><a class="tocitem" href="#Second-order-optimizers"><span>Second-order optimizers</span></a></li><li><a class="tocitem" href="#Natural-gradient"><span>Natural gradient</span></a></li></ul></li><li><a class="tocitem" href="../examples/">Examples</a></li><li><span class="tocitem">Qiskit Wrapper</span><ul><li><a class="tocitem" href="../qiskit/intro/">Introduction</a></li><li><a class="tocitem" href="../qiskit/optimizers/">Optimizers</a></li></ul></li><li><a class="tocitem" href="../tools/">Other tools</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Optimizers</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Optimizers</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/jgidi/ComplexSPSA.jl/blob/master/docs/src/optimizers.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Optimizers"><a class="docs-heading-anchor" href="#Optimizers">Optimizers</a><a id="Optimizers-1"></a><a class="docs-heading-anchor-permalink" href="#Optimizers" title="Permalink"></a></h1><p>This package provides a set optimizers for real-valued functions of a number of complex variables, <span>$f:\mathbb{C}^N\to\mathbb{R}$</span>, which we subdivide into three categories</p><h2 id="First-order-optimizers"><a class="docs-heading-anchor" href="#First-order-optimizers">First-order optimizers</a><a id="First-order-optimizers-1"></a><a class="docs-heading-anchor-permalink" href="#First-order-optimizers" title="Permalink"></a></h2><p>To reach a local minimum of <span>$f(\bm z)$</span> with <span>$\bm z \in \mathbb{C}^N$</span>, this methods start from some seed value of <span>$\bm z = \bm z^0$</span> and iterate over <span>$k$</span> performing a first-order update,</p><p class="math-container">\[\bm z^{k+1} = \bm z^k - a^k \bm g^{k}(\bm z^k, b^k),\]</p><p>where <span>$\bm g^k(\bm z^k, b^k)$</span> is a randomly directed finite-difference estimate of the gradient of <span>$f$</span>, <span>$\partial f / \partial\bm{z}$</span>, and</p><p class="math-container">\[\begin{aligned}
a^k &amp;= \frac{a}{(k + A + 1)^s}, \\
b^k &amp;= \frac{b}{(k + 1)^t},
\end{aligned}\]</p><p>are convergence factors determined by the input gain parameters <span>$a$</span>, <span>$b$</span>, <span>$A$</span>, <span>$s$</span>, and <span>$t$</span>.</p><p>By default, standard gains are defined and used package-wide from the dictionary <a href="#ComplexSPSA.gains"><code>ComplexSPSA.gains</code></a>,</p><article class="docstring"><header><a class="docstring-binding" id="ComplexSPSA.gains" href="#ComplexSPSA.gains"><code>ComplexSPSA.gains</code></a> — <span class="docstring-category">Constant</span></header><section><div><pre><code class="language-julia hljs">gains = Dict(
    :a =&gt; 3.0,
    :b =&gt; 0.1,
    :A =&gt; 0.0,
    :s =&gt; 0.602,
    :t =&gt; 0.101,
)</code></pre><p>Contains the gain parameters used for the optimizers defined within the <code>ComplexSPSA</code> module. By default, the standard gains are used.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jgidi/ComplexSPSA.jl/blob/ea41a9f87d937c77c624f7365511b61d9ad6efb4/src/include/gains.jl#L21-L33">source</a></section></article><p>where they can be changed on run-time, and the optimizers will use their value as defined at the moment when they are called.</p><h3 id="SPSA-on-complex"><a class="docs-heading-anchor" href="#SPSA-on-complex">SPSA on complex</a><a id="SPSA-on-complex-1"></a><a class="docs-heading-anchor-permalink" href="#SPSA-on-complex" title="Permalink"></a></h3><p>In particular, two first-order methods are implemented. The first, called <a href="#ComplexSPSA.SPSA_on_complex"><code>SPSA_on_complex</code></a>, is based on the <a href="https://www.jhuapl.edu/spsa/">SPSA optimization method</a> for real variables of real functions, and works by treating each complex variable as a pair of real variables. This method is commonly used for the minimization of real functions of complex variables, but may result cumbersome on domains where the complex algebra is natively involved.</p><article class="docstring"><header><a class="docstring-binding" id="ComplexSPSA.SPSA_on_complex" href="#ComplexSPSA.SPSA_on_complex"><code>ComplexSPSA.SPSA_on_complex</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">SPSA_on_complex(f::Function, z₀::Vector, Niters = 200;
                sign = -1,
                Ncalibrate = 0,
                initial_iteration = 1,
                constant_learning_rate = false,
                a = gains[:a], b = gains[:b],
                A = gains[:A], s = gains[:s], t = gains[:t],
                )</code></pre><p>The <a href="https://www.jhuapl.edu/spsa/">Simultaneous Perturbation Stochastic Approximation (SPSA)</a> optimizer is a stochastic method for optimizing real functions of a number of real variables.</p><p>This function performs SPSA optimization of the real-valued function <code>f</code> of complex variables by treating each complex varible as a pair of real variables, starting from the complex vector <code>z₀</code> and iterating <code>Niter</code> times. Then, returns a complex matrix, <code>zacc</code>, with size <code>(length(z₀), Niters)</code>, such that <code>zacc[i, j]</code> corresponds to the value of the <code>i</code>-th complex variable on the <code>j</code>-th iteration.</p><p>The input parameters <code>a</code>, <code>b</code>, <code>A</code>, <code>s</code>, and <code>t</code> can be provided as keyword arguments of the function. If they are not provided explicitly, they are selected at runtime from the <a href="#ComplexSPSA.gains"><code>ComplexSPSA.gains</code></a> dictionary.</p><p>Automatic calibration for the input parameter <code>a</code> can be accomplished taking <code>Ncalibrate</code> samples using the method defined on <a href="https://arxiv.org/pdf/1704.05018v2.pdf#section*.11">Kandala <em>et. al.</em> (2017), Sec. 11</a>. By default, the calibration is disabled (<code>Ncalibrate = 0</code>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jgidi/ComplexSPSA.jl/blob/ea41a9f87d937c77c624f7365511b61d9ad6efb4/src/include/first_order.jl#L1-L23">source</a></section></article><h3 id="CSPSA"><a class="docs-heading-anchor" href="#CSPSA">CSPSA</a><a id="CSPSA-1"></a><a class="docs-heading-anchor-permalink" href="#CSPSA" title="Permalink"></a></h3><p>A second method called <a href="#ComplexSPSA.CSPSA"><code>CSPSA</code></a> is also provided, which has the advantage of having being formulated in terms of complex variables, thus resulting more natural in natively-complex domains.</p><article class="docstring"><header><a class="docstring-binding" id="ComplexSPSA.CSPSA" href="#ComplexSPSA.CSPSA"><code>ComplexSPSA.CSPSA</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">CSPSA(f::Function, z₀::Vector, Niters = 200;
      sign = -1,
      Ncalibrate = 0,
      initial_iteration = 1,
      constant_learning_rate = false,
      a = gains[:a], b = gains[:b],
      A = gains[:A], s = gains[:s], t = gains[:t],
      )</code></pre><p>The <a href="https://www.nature.com/articles/s41598-019-52289-0">Complex Simultaneous Perturbation Stochastic Approximation (CSPSA)</a> optimizer is a method for optimizing real functions of a number of complex variables, and corresponds to a complex generalization of the <a href="https://www.jhuapl.edu/spsa/">SPSA method</a>.</p><p>This function performs CSPSA optimization of the real-valued function <code>f</code>, starting from a vector of complex variables <code>z₀</code> and iterating <code>Niter</code> times. Then, returns a complex matrix, <code>zacc</code>, with size <code>(length(z₀), Niters)</code>, such that <code>zacc[i, j]</code> corresponds to the value of the <code>i</code>-th complex variable on the <code>j</code>-th iteration.</p><p>The input parameters <code>a</code>, <code>b</code>, <code>A</code>, <code>s</code>, and <code>t</code> can be provided as keyword arguments of the function. If they are not provided explicitly, they are selected at runtime from the <a href="#ComplexSPSA.gains"><code>ComplexSPSA.gains</code></a> dictionary.</p><p>Automatic calibration for the input parameter <code>a</code> can be accomplished taking <code>Ncalibrate</code> samples using the method defined on <a href="https://arxiv.org/pdf/1704.05018v2.pdf#section*.11">Kandala <em>et. al.</em> (2017), Sec. 11</a>. By default, the calibration is disabled (<code>Ncalibrate = 0</code>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jgidi/ComplexSPSA.jl/blob/ea41a9f87d937c77c624f7365511b61d9ad6efb4/src/include/first_order.jl#L87-L110">source</a></section></article><h2 id="Second-order-optimizers"><a class="docs-heading-anchor" href="#Second-order-optimizers">Second-order optimizers</a><a id="Second-order-optimizers-1"></a><a class="docs-heading-anchor-permalink" href="#Second-order-optimizers" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ComplexSPSA.SPSA2_on_complex" href="#ComplexSPSA.SPSA2_on_complex"><code>ComplexSPSA.SPSA2_on_complex</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">SPSA2_on_complex(f::Function, z₀::Vector, Niters = 200;
                 sign = -1,
                 hessian_delay = 0,
                 initial_iteration = 1,
                 constant_learning_rate = false,
                 a = gains[:a], b = gains[:b],
                 A = gains[:A], s = gains[:s], t = gains[:t],
                 )</code></pre><p>The second-order SPSA, commonly referred to as <code>2-SPSA</code> method is a second-order stochastic optimization method based on <a href="https://www.jhuapl.edu/spsa/">SPSA</a>, which additional to a gradient estimate performs a Hessian correction on the update rule to optimize real-valued functions of a number of real variables.</p><p>This function performs second-order SPSA optimization of the real-valued function <code>f</code> of complex variables by treating each complex variable as a pair of real variables, starting from the complex vector <code>z₀</code> and iterating <code>Niter</code> times. Then, returns a complex matrix, <code>zacc</code>, with size <code>(length(z₀), Niters)</code>, such that <code>zacc[i, j]</code> corresponds to the value of the <code>i</code>-th complex variable on the <code>j</code>-th iteration.</p><p>The input parameters <code>a</code>, <code>b</code>, <code>A</code>, <code>s</code>, and <code>t</code> can be provided as keyword arguments of the function. If not provided explicitly, they are selected at runtime from the <a href="#ComplexSPSA.gains"><code>ComplexSPSA.gains</code></a> dictionary.</p><p>Since second-order effects usually show improvements once the seed value is closer to a local minimum, it is possible to accept a number <code>hessian_delay</code> of first-order iterations before including the application of the Hessian information.</p><p><strong>Notes</strong></p><ul><li>The value of <code>a</code> is only required to perform a possible number of initial first-order iterations (via <code>hessian_delay</code>), since the second-order iterations yield an optimum value for <code>a = 1</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jgidi/ComplexSPSA.jl/blob/ea41a9f87d937c77c624f7365511b61d9ad6efb4/src/include/second_order.jl#L1-L28">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexSPSA.CSPSA2" href="#ComplexSPSA.CSPSA2"><code>ComplexSPSA.CSPSA2</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">CSPSA2(f::Function, z₀::Vector, Niters = 200;
       sign = -1,
       hessian_delay = 0,
       initial_iteration = 1,
       constant_learning_rate = false,
       a = gains[:a], b = gains[:b],
       A = gains[:A], s = gains[:s], t = gains[:t],
       )</code></pre><p>The second-order CSPSA method, CSPSA2, is a second-order stochastic optimization method based on <a href="#ComplexSPSA.CSPSA"><code>CSPSA</code></a>, which additional to a gradient estimate performs a Hessian correction on the update rule to optimize real-valued functions of a number of complex variables.</p><p>This function performs second-order CSPSA optimization of the real-valued function <code>f</code> of complex variables, starting from the complex vector <code>z₀</code> and iterating <code>Niter</code> times. Then, returns a complex matrix, <code>zacc</code>, with size <code>(length(z₀), Niters)</code>, such that <code>zacc[i, j]</code> corresponds to the value of the <code>i</code>-th complex variable on the <code>j</code>-th iteration.</p><p>The input parameters <code>a</code>, <code>b</code>, <code>A</code>, <code>s</code>, and <code>t</code> can be provided as keyword arguments of the function. If they are not provided explicitly, they are selected at runtime from the <a href="#ComplexSPSA.gains"><code>ComplexSPSA.gains</code></a> dictionary.</p><p>Since second-order effects usually show improvements once the seed value is closer to a local minimum, it is possible to accept a number <code>hessian_delay</code> of first-order iterations before including the application of the Hessian information.</p><p><strong>Notes</strong></p><ul><li>The value of <code>a</code> is only required to perform a possible number of initial first-order iterations (via <code>hessian_delay</code>), since the second-order iterations yield an optimum value for <code>a = 1</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jgidi/ComplexSPSA.jl/blob/ea41a9f87d937c77c624f7365511b61d9ad6efb4/src/include/second_order.jl#L116-L143">source</a></section></article><h2 id="Natural-gradient"><a class="docs-heading-anchor" href="#Natural-gradient">Natural gradient</a><a id="Natural-gradient-1"></a><a class="docs-heading-anchor-permalink" href="#Natural-gradient" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ComplexSPSA.SPSA_QN_on_complex" href="#ComplexSPSA.SPSA_QN_on_complex"><code>ComplexSPSA.SPSA_QN_on_complex</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">SPSA_QN_on_complex(f::Function, z₀::Vector, Niters = 200;
                   sign = -1,
                   hessian_delay = 0,
                   initial_iteration = 1,
                   constant_learning_rate = false,
                   a = gains[:a], b = gains[:b],
                   A = gains[:A], s = gains[:s], t = gains[:t],
                   )</code></pre><p>The Quantum Natural SPSA, presented by <a href="https://arxiv.org/abs/2103.09232">Gacon <em>et. al</em>. (2021)</a>, is a second-order stochastic optimization method based on <a href="https://www.jhuapl.edu/spsa/">2-SPSA</a>, where the second-order correction comes from the Hessian of the Fubini-Study metric of the problem instead of the Hessian of the function under optimization.</p><p>Note that the metric must be a function taking two input vectors, and returning minus a half of the fidelity between the states generated each from one input,</p><p>$ \text{metric}(\vec z₁, \vec z₂) = -\frac{1}{2} |\langle ψ(\vec z₁) | ψ(\vec z₂) \rangle|^2, $</p><p>where $ ψ(\vec z) $ is the quantum state parameterized with the variables $ \vec z $.</p><p>This function performs Quantum Natural SPSA optimization of the real-valued function <code>f</code> of complex variables by treating each complex variable as a pair of real variables, starting from the complex vector <code>z₀</code> and iterating <code>Niter</code> times. Then, returns a complex matrix, <code>zacc</code>, with size <code>(length(z₀), Niters)</code>, such that <code>zacc[i, j]</code> corresponds to the value of the <code>i</code>-th complex variable on the <code>j</code>-th iteration.</p><p>The input parameters <code>a</code>, <code>b</code>, <code>A</code>, <code>s</code>, and <code>t</code> can be provided as keyword arguments of the function. If not provided explicitly, they are selected at runtime from the <a href="#ComplexSPSA.gains"><code>ComplexSPSA.gains</code></a> dictionary.</p><p>Since second-order effects usually show improvements once the seed value is closer to a local minimum, it is possible to accept a number <code>hessian_delay</code> of first-order iterations before including the application of the Hessian information.</p><p><strong>Notes</strong></p><ul><li>The value of <code>a</code> is only required to perform a possible number of initial first-order iterations (via <code>hessian_delay</code>), since the second-order iterations yield an optimum value for <code>a = 1</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jgidi/ComplexSPSA.jl/blob/ea41a9f87d937c77c624f7365511b61d9ad6efb4/src/include/natural_gradient.jl#L1-L35">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexSPSA.CSPSA_QN" href="#ComplexSPSA.CSPSA_QN"><code>ComplexSPSA.CSPSA_QN</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">CSPSA_QN(f::Function, metric::Function, z₀::Vector, Niters = 200;
         sign = -1,
         hessian_delay = 0,
         initial_iteration = 1,
         constant_learning_rate = false,
         a = gains[:a], b = gains[:b],
         A = gains[:A], s = gains[:s], t = gains[:t],
         )</code></pre><p>The Quantum Natural CSPSA (QN-CSPSA), is a second-order stochastic optimization method which, analogous to the <a href="https://arxiv.org/abs/2103.09232">Quantum Natural SPSA by Gacon <em>et. al</em>. (2021)</a>, takes into account a stochastic approximation of the Fubiny-Study metric instead of the usual Hessian correction from <a href="#ComplexSPSA.CSPSA2"><code>CSPSA2</code></a>. However, the main difference between QN-CSPSA and QN-SPSA is that the former is natively formulated in terms of complex variables, while the latter requires real variables. Note that the metric must be a function taking two input vectors, and returning minus a half of the fidelity between the states generated each from one input,</p><p>$ \text{metric}(\vec z₁, \vec z₂) = -\frac{1}{2} |\langle ψ(\vec z₁) | ψ(\vec z₂) \rangle|^2, $</p><p>where $ ψ(\vec z) $ is the quantum state parameterized with the variables $ \vec z $.</p><p>This function performs Quantum Natural CSPSA optimization of the real-valued function <code>f</code> of complex variables, starting from the complex vector <code>z₀</code> and iterating <code>Niter</code> times. Then, returns a complex matrix, <code>zacc</code>, with size <code>(length(z₀), Niters)</code>, such that <code>zacc[i, j]</code> corresponds to the value of the <code>i</code>-th complex variable on the <code>j</code>-th iteration.</p><p>The input parameters <code>a</code>, <code>b</code>, <code>A</code>, <code>s</code>, and <code>t</code> can be provided as keyword arguments of the function. If not provided explicitly, they are selected at runtime from the <a href="#ComplexSPSA.gains"><code>ComplexSPSA.gains</code></a> dictionary.</p><p>Since second-order effects usually show improvements once the seed value is closer to a local minimum, it is possible to accept a number <code>hessian_delay</code> of first-order iterations before including the application of the Hessian information.</p><p><strong>Notes</strong></p><ul><li>The value of <code>a</code> is only required to perform a possible number of initial first-order iterations (via <code>hessian_delay</code>), since the second-order iterations yield an optimum value for <code>a = 1</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jgidi/ComplexSPSA.jl/blob/ea41a9f87d937c77c624f7365511b61d9ad6efb4/src/include/natural_gradient.jl#L123-L156">source</a></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>SPSA_QN_scalar_on_complex</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>CSPSA_QN_scalar</code>. Check Documenter&#39;s build log for details.</p></div></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Introduction</a><a class="docs-footer-nextpage" href="../examples/">Examples »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.6 on <span class="colophon-date" title="Tuesday 30 August 2022 20:18">Tuesday 30 August 2022</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
